# -*- coding: utf-8 -*-
"""mnist4beginners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zu8zCh9Sm_xu8X6QMjiUdjZVTbsKLZu3
"""

# Commented out IPython magic to ensure Python compatibility.

'''
Code writen by Dweep Joshipura
Blog - lightspeedac.blogspot.com
Please read the comments carefully
'''
# Go to Edit->Notebook Settings->Hardware Accelerator->Select GPU and Save
# Doing the above will speed up the training treemendously and Save time.
#Importing Keras programming framework
import keras 
#Ensuring that TensorFlow v1 is used as backend
# %tensorflow_version 1.x 
#Importing kinds of layers
from keras.layers import Dense,Dropout
#Importing a basic model type
from keras.models import Sequential
#Importing the DIGIT CLASSIFIER dataset 
from keras.datasets import mnist 
#Importing a function used later
from keras.utils import to_categorical

def mnist_preprocess(X):
  '''
  This function takes the Inputs and Reshapes them into 2 dim matrices
  It also normalizes the Input
  '''
  X = X.reshape((X.shape[0],X.shape[1]*X.shape[2]))
  X = X.astype('float32')
  X /= 255
  return X

# the number can be of 10 types (0,1,2,3,4,5,6,7,8,9)
num_classes = 10
'''
Here-
X = input for Neural Network
Y = ground truth labels for input X for NN to train on
X_test = testing inputs we will use after training
Y_test = ground truth labels for input X_test to evaluate NN.
The dataset MNIST has 60000 training examples and 10000 test examples.
'''
(X,Y),(X_test,Y_test) = mnist.load_data()
# Preprocesses the data
X = mnist_preprocess(X)
X_test = mnist_preprocess(X_test)
# Preprocesses Y
Y = to_categorical(Y,num_classes)
Y_test = to_categorical(Y_test,num_classes)

# As the name suggests, the Sequential model uses sequence of layers to automatically create NN
model = Sequential(name="my-first-NN")
'''
The first type of layer is the one we've talked about in blog.
We are having one layer consisting of 32 units computing the same function, although with different parameters.
This will enable more complexity.
The activation is ReLU, which is just a graph like _/ which eliminates negative numbers by giving them value of zero.
To automatically build a model, the first layer needs to be given the shape of the input.
'''
model.add(Dense(units=64,activation='relu',input_shape=(X.shape[1],)))
'''
Another type of layer is Dropout, which, to increase robustness randomly eliminates some percentage of units.
This will enable better accuracy and training speed
The rate is a parameter which we'll have to set. rate = 0.5 will mean that half of the units shall be randomly eliminated
'''
model.add(Dropout(rate=0.2))
# We will add few more group of layers like this one. You may play around with num of units and dropout rate
model.add(Dense(units=512,activation='relu'))
model.add(Dropout(rate=0.2))
model.add(Dense(units=512,activation='relu'))
model.add(Dropout(rate=0.2))
model.add(Dense(units=64,activation='relu'))
model.add(Dropout(rate=0.2))
'''
The problem we are currently tackling, requires multiple 0 or 1 outputs.
Such 'multi-class classification problems' use softmax function
NOTE: As we have ten classes the units = num_classes shouldn't be changed
'''
model.add(Dense(units = num_classes, activation = 'softmax'))
#In the summary, you can see that there are lot of params!!
model.summary()
'''This will specify the loss as categorical loss and optimizer as Adam
Details about Adam are beyond the scope of this blog'''
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
# This will train our models' all examples 20 times
model.fit(x = X,y = Y,epochs = 20,verbose = 0)
score = model.evaluate(X_test,Y_test,verbose = 0)
print('Test accuracy = '+ str(score[1]))
